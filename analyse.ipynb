{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0317e294",
   "metadata": {},
   "source": [
    "# Analyse des outputs de CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b6a757",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def import_data(file_path):\n",
    "    try:\n",
    "        data = pd.read_csv(file_path)\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {file_path} not found.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "df_real = import_data('./scv_1/val_labels.csv')\n",
    "df_real[\"file\"] = df_real[\"file\"].str.replace('val/', '')\n",
    "df_real.rename(columns={\"file\": \"image\"}, inplace=True)\n",
    "df_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6caab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "def plot_comparison(df: pd.DataFrame,\n",
    "                    base_column: str,\n",
    "                    pred_suffix: str = '_pred',\n",
    "                    true_suffix: str = '_true',\n",
    "                    title: str = None) -> go.Figure:\n",
    "    \n",
    "    pred_col = f\"{base_column}{pred_suffix}\"\n",
    "    true_col = f\"{base_column}{true_suffix}\"\n",
    "\n",
    "    \n",
    "    if pred_col not in df.columns or true_col not in df.columns:\n",
    "        raise ValueError(f\"Columns '{pred_col}' and/or '{true_col}' not found in DataFrame.\")\n",
    "\n",
    "    pred_counts = df[pred_col].value_counts().sort_index()\n",
    "    true_counts = df[true_col].value_counts().sort_index()\n",
    "    fig = make_subplots(rows=1, cols=2,\n",
    "                        subplot_titles=(f\"{base_column.capitalize()} Predicted\", f\"{base_column.capitalize()} True\"))\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=pred_counts.index.astype(str), y=pred_counts.values, name='Predicted'),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=true_counts.index.astype(str), y=true_counts.values, name='True'),\n",
    "        row=1, col=2\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title_text=title or f\"Comparison of '{pred_col}' vs '{true_col}' distributions\",\n",
    "        showlegend=False,\n",
    "        width=800,\n",
    "        height=400\n",
    "    )\n",
    "\n",
    "    fig.update_xaxes(title_text=base_column.capitalize(), row=1, col=1)\n",
    "    fig.update_xaxes(title_text=base_column.capitalize(), row=1, col=2)\n",
    "    fig.update_yaxes(title_text='Count', row=1, col=1)\n",
    "    fig.update_yaxes(title_text='Count', row=1, col=2)\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def calculate_error_across_classes(df: pd.DataFrame,\n",
    "                                      base_column: str,\n",
    "                                      pred_suffix: str = '_pred',\n",
    "                                      true_suffix: str = '_true') -> None:\n",
    "    \n",
    "    pred_col = f\"{base_column}{pred_suffix}\"\n",
    "    true_col = f\"{base_column}{true_suffix}\"\n",
    "\n",
    "    if pred_col not in df.columns or true_col not in df.columns:\n",
    "        raise ValueError(f\"Columns '{pred_col}' and/or '{true_col}' not found in DataFrame.\")\n",
    "\n",
    "    accuracy_per_class = (df[pred_col] != df[true_col]).groupby(df[true_col]).mean()\n",
    "    print(\"Global Error Rate: \", accuracy_per_class.mean().round(2))\n",
    "    fig = go.Figure(\n",
    "        data=[\n",
    "            go.Bar(\n",
    "                x=accuracy_per_class.index.astype(str),\n",
    "                y=accuracy_per_class.values,\n",
    "                marker=dict(color='skyblue'),\n",
    "                text=accuracy_per_class.values.round(2),\n",
    "                textposition='auto'\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=f'Error rate per Class for {base_column.capitalize()}',\n",
    "        xaxis_title=f'{base_column.capitalize()} Class',\n",
    "        yaxis_title='Error Rate',\n",
    "        xaxis=dict(tickmode='linear'),\n",
    "        height=400,\n",
    "        width=600\n",
    "    )\n",
    "    return fig\n",
    "\n",
    "def confusion_matrix(df: pd.DataFrame,\n",
    "                     base_column: str,\n",
    "                     pred_suffix: str = '_pred',\n",
    "                     true_suffix: str = '_true') -> None:\n",
    "    pred_col = f\"{base_column}{pred_suffix}\"\n",
    "    true_col = f\"{base_column}{true_suffix}\"\n",
    "\n",
    "    if pred_col not in df.columns or true_col not in df.columns:\n",
    "        raise ValueError(f\"Columns '{pred_col}' and/or '{true_col}' not found in DataFrame.\")\n",
    "\n",
    "    # Create the confusion matrix\n",
    "    confusion = pd.crosstab(df[true_col], df[pred_col], rownames=['True'], colnames=['Predicted'], margins=False)\n",
    "\n",
    "    # Add annotations (numbers) to each square\n",
    "    annotations = [[f\"{value}\" for value in row] for row in confusion.values]\n",
    "\n",
    "    # Create the heatmap\n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "        z=confusion.values,\n",
    "        x=confusion.columns.astype(str),\n",
    "        y=confusion.index.astype(str),\n",
    "        colorscale='Blues',  # Change the color mapping here\n",
    "        text=annotations,  # Add annotations\n",
    "        texttemplate=\"%{text}\",  # Display annotations\n",
    "        hoverinfo=\"z\"  # Show only the value on hover\n",
    "    ))\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title=f'Confusion Matrix for {base_column.capitalize()}',\n",
    "        xaxis_title='Predicted Class',\n",
    "        yaxis_title='True Class',\n",
    "        height=600,\n",
    "        width=600\n",
    "    )\n",
    "\n",
    "    return fig\n",
    "\n",
    "def run_stats(preds, cat):\n",
    "    df_merged = pd.merge(preds, df_real, on='image', suffixes=('_pred', '_true'))\n",
    "    plot_comparison(df_merged, cat , title=  f'{cat} Distribution Comparison').show()\n",
    "    calculate_error_across_classes(df_merged, cat).show()\n",
    "    confusion_matrix(df_merged, cat).show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37416113",
   "metadata": {},
   "source": [
    "## Ethnie"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac636b16",
   "metadata": {},
   "source": [
    "#### Sans contexte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21a6c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_r_sans = import_data('./scv_1/r_sans.csv')\n",
    "run_stats(df_r_sans, \"race\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1193b24d",
   "metadata": {},
   "source": [
    "#### C1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8933f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_r_sans = import_data('./scv_1/r_c1.csv')\n",
    "run_stats(df_r_sans, \"race\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688e9edc",
   "metadata": {},
   "source": [
    "#### C2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a62df59",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_r_sans = import_data('./scv_1/r_c2.csv')\n",
    "run_stats(df_r_sans, \"race\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2eeeff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
