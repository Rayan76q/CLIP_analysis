{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc6974e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd48fc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    df_summary = pd.read_csv(\"./medical/data_summary.csv\")\n",
    "    df_summary[\"filename\"] = df_summary[\"filename\"].apply(lambda x: x.split(\".\")[0])\n",
    "    df_summary.head()\n",
    "\n",
    "    FOLDER_PATH = path\n",
    "\n",
    "\n",
    "    IMAGE_EXTENSIONS = ['.png', '.jpg', '.jpeg']\n",
    "    data_list = []\n",
    "\n",
    "    for filename in os.listdir(FOLDER_PATH):\n",
    "        if filename.endswith('.npz'):\n",
    "            file_stem = os.path.splitext(filename)[0]\n",
    "            npz_path = os.path.join(FOLDER_PATH, filename)\n",
    "            npz_data = np.load(npz_path, allow_pickle=True)\n",
    "            id = file_stem.split(\"_\")[-1]\n",
    "\n",
    "\n",
    "            data_entry = {\n",
    "                \"filename\": file_stem,\n",
    "                \"slo_fundus\": npz_data[\"slo_fundus\"],\n",
    "            }\n",
    "\n",
    "            data_list.append(data_entry)\n",
    "\n",
    "    print(f\"Loaded {len(data_list)} entries successfully.\")\n",
    "    df = pd.DataFrame(data_list)\n",
    "    df = pd.merge(df, df_summary, on='filename', how='inner').drop(columns = [\"use\"])\n",
    "    return df\n",
    "\n",
    "df_train = load_data(\"./medical/Training\")\n",
    "df_test = load_data(\"./medical/Test\")\n",
    "df_val = load_data(\"./medical/Validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c3b99b",
   "metadata": {},
   "source": [
    "# Analyse de la distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2409e854",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "def plot_distributions(df_train, df_test, df_val):\n",
    "    df_train['age_rounded'] = df_train['age'].round()\n",
    "    df_test['age_rounded'] = df_test['age'].round()\n",
    "    df_val['age_rounded'] = df_val['age'].round()\n",
    "\n",
    "    fig = make_subplots(\n",
    "        rows=7, cols=3,\n",
    "        subplot_titles=[\n",
    "            \"Race Distribution (Train)\", \"Race Distribution (Test)\", \"Race Distribution (Val)\",\n",
    "            \"Gender Distribution (Train)\", \"Gender Distribution (Test)\", \"Gender Distribution (Val)\",\n",
    "            \"Age Distribution (Train)\", \"Age Distribution (Test)\", \"Age Distribution (Val)\",\n",
    "            \"Ethnicity Distribution (Train)\", \"Ethnicity Distribution (Test)\", \"Ethnicity Distribution (Val)\",\n",
    "            \"Language Distribution (Train)\", \"Language Distribution (Test)\", \"Language Distribution (Val)\",\n",
    "            \"Marital Status Distribution (Train)\", \"Marital Status Distribution (Test)\", \"Marital Status Distribution (Val)\",\n",
    "            \"Healthy vs Sick (Train)\", \"Healthy vs Sick (Test)\", \"Healthy vs Sick (Val)\"\n",
    "        ],\n",
    "        specs=[\n",
    "            [{\"type\": \"pie\"}, {\"type\": \"pie\"}, {\"type\": \"pie\"}],\n",
    "            [{\"type\": \"pie\"}, {\"type\": \"pie\"}, {\"type\": \"pie\"}],\n",
    "            [{\"type\": \"scatter\"}, {\"type\": \"scatter\"}, {\"type\": \"scatter\"}],\n",
    "            [{\"type\": \"pie\"}, {\"type\": \"pie\"}, {\"type\": \"pie\"}],\n",
    "            [{\"type\": \"pie\"}, {\"type\": \"pie\"}, {\"type\": \"pie\"}],\n",
    "            [{\"type\": \"pie\"}, {\"type\": \"pie\"}, {\"type\": \"pie\"}],\n",
    "            [{\"type\": \"pie\"}, {\"type\": \"pie\"}, {\"type\": \"pie\"}]\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    for i, (df, title) in enumerate(zip([df_train, df_test, df_val], [\"Train\", \"Test\", \"Val\"])):\n",
    "        race_counts = df['race'].value_counts()\n",
    "        fig.add_trace(go.Pie(labels=race_counts.index, values=race_counts.values, name=f\"Race {title}\"), row=1, col=i + 1)\n",
    "\n",
    "        gender_counts = df['gender'].value_counts()\n",
    "        fig.add_trace(go.Pie(labels=gender_counts.index, values=gender_counts.values, name=f\"Gender {title}\"), row=2, col=i + 1)\n",
    "\n",
    "        age_counts = df['age_rounded'].value_counts().sort_index()\n",
    "        fig.add_trace(go.Scatter(x=age_counts.index, y=age_counts.values, mode='lines+markers', name=f\"Age {title}\"), row=3, col=i + 1)\n",
    "\n",
    "        ethnicity_counts = df['ethnicity'].value_counts()\n",
    "        fig.add_trace(go.Pie(labels=ethnicity_counts.index, values=ethnicity_counts.values, name=f\"Ethnicity {title}\"), row=4, col=i + 1)\n",
    "\n",
    "        language_counts = df['language'].value_counts()\n",
    "        fig.add_trace(go.Pie(labels=language_counts.index, values=language_counts.values, name=f\"Language {title}\"), row=5, col=i + 1)\n",
    "\n",
    "        marital_status_counts = df['maritalstatus'].value_counts()\n",
    "        fig.add_trace(go.Pie(labels=marital_status_counts.index, values=marital_status_counts.values, name=f\"Marital Status {title}\"), row=6, col=i + 1)\n",
    "\n",
    "        glaucoma_counts = df['glaucoma'].value_counts()\n",
    "        fig.add_trace(go.Pie(labels=glaucoma_counts.index, values=glaucoma_counts.values, name=f\"Healthy vs Sick {title}\"), row=7, col=i + 1)\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=\"Comparison of Distributions Across Datasets\",\n",
    "        height=2100,  # Adjusted height for 7 rows\n",
    "        width=1200,\n",
    "        showlegend=False\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "plot_distributions(df_train, df_test, df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52950277",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = pd.concat([df_train, df_test, df_val], axis=0, ignore_index=True)\n",
    "df_combined['dataset'] = ['Train'] * len(df_train) + ['Test'] * len(df_test) + ['Val'] * len(df_val)\n",
    "df_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc198078",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.sklearn.metrics import *\n",
    "\n",
    "\n",
    "def get_group_metrics(\n",
    "    y_true,\n",
    "    y_pred=None,\n",
    "    prot_attr=None,\n",
    "    priv_group=1,\n",
    "    pos_label=1,\n",
    "    sample_weight=None,\n",
    "):\n",
    "    group_metrics = {}\n",
    "    group_metrics[\"base_rate\"] = base_rate(\n",
    "        y_true=y_true, pos_label=pos_label, sample_weight=sample_weight\n",
    "    )\n",
    "    group_metrics[\"statistical_parity_difference\"] = statistical_parity_difference(\n",
    "        y_true=y_true, y_pred=y_pred, prot_attr=prot_attr, priv_group=priv_group, pos_label=pos_label, sample_weight=sample_weight\n",
    "    )\n",
    "    group_metrics[\"disparate_impact_ratio\"] = disparate_impact_ratio(\n",
    "        y_true=y_true, y_pred=y_pred, prot_attr=prot_attr, priv_group=priv_group, pos_label=pos_label, sample_weight=sample_weight\n",
    "    )\n",
    "    if not y_pred is None:\n",
    "        group_metrics[\"equal_opportunity_difference\"] = equal_opportunity_difference(\n",
    "            y_true=y_true, y_pred=y_pred, prot_attr=prot_attr, priv_group=priv_group, pos_label=pos_label, sample_weight=sample_weight\n",
    "        )\n",
    "        group_metrics[\"average_odds_difference\"] = average_odds_difference(\n",
    "            y_true=y_true, y_pred=y_pred, prot_attr=prot_attr, priv_group=priv_group, pos_label=pos_label, sample_weight=sample_weight\n",
    "        )\n",
    "        group_metrics[\"conditional_demographic_disparity\"] = conditional_demographic_disparity(\n",
    "            y_true=y_true, y_pred=y_pred, prot_attr=prot_attr, pos_label=pos_label, sample_weight=sample_weight\n",
    "        )\n",
    "        group_metrics[\"smoothed_edf\"] = smoothed_edf(\n",
    "        y_true=y_true, y_pred=y_pred, prot_attr=prot_attr, pos_label=pos_label, sample_weight=sample_weight\n",
    "        )\n",
    "        group_metrics[\"df_bias_amplification\"] = df_bias_amplification(\n",
    "        y_true=y_true, y_pred=y_pred, prot_attr=prot_attr, pos_label=pos_label, sample_weight=sample_weight\n",
    "        )\n",
    "    return group_metrics\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ideal_values = {\n",
    "    \"statistical_parity_difference\": 0.0,\n",
    "    \"disparate_impact_ratio\": 1.0,\n",
    "    \"equal_opportunity_difference\": 0.0,\n",
    "    \"average_odds_difference\": 0.0,\n",
    "    \"conditional_demographic_disparity\": 0.0,\n",
    "    \"smoothed_edf\": 1.0,\n",
    "    \"df_bias_amplification\": 0.0\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "def check_distance_to_ideal(metrics):\n",
    "    dist = 0.0\n",
    "    for metric in ideal_values.keys():\n",
    "        dist += (ideal_values[metric]-metrics[metric])**2\n",
    "    return np.sqrt(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfeea2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "\n",
    "df_test_numeric = df_test.drop(columns=['filename', 'slo_fundus', 'age', 'note', 'gpt4_summary'])\n",
    "\n",
    "def convert_to_numerical_with_encodings(df):\n",
    "    df_numeric = df.copy()\n",
    "    encodings = {}\n",
    "    for col in df_numeric.columns:\n",
    "        if df_numeric[col].dtype == 'object':  # Check if the column is categorical\n",
    "            le = LabelEncoder()\n",
    "            df_numeric[col] = le.fit_transform(df_numeric[col])\n",
    "            encodings[col] = dict(zip(le.classes_, le.transform(le.classes_)))  # Store the encoding\n",
    "    return df_numeric, encodings\n",
    "\n",
    "\n",
    "df_test_numeric, encodings = convert_to_numerical_with_encodings(df_test_numeric)\n",
    "\n",
    "\n",
    "def create_binary_label_dataset(df, label_column, protected_attributes):\n",
    "    return BinaryLabelDataset(\n",
    "        df=df,\n",
    "        label_names=[label_column],\n",
    "        protected_attribute_names=protected_attributes\n",
    "    )\n",
    "\n",
    "\n",
    "label_column = 'glaucoma'  \n",
    "protected_attributes = ['gender', 'race', 'ethnicity', 'language', 'maritalstatus']  \n",
    "\n",
    "\n",
    "dataset = create_binary_label_dataset(df_test_numeric, label_column, protected_attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c347e63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f43137",
   "metadata": {},
   "outputs": [],
   "source": [
    "priviliged_groups = [0,2,1,0,2]\n",
    "protected_attributes = dataset.protected_attributes\n",
    "y_true = dataset.labels\n",
    "\n",
    "\n",
    "n_prot_attrs = protected_attributes.shape[1]\n",
    "attribute_names = ['gender', 'race', 'ethnicity', 'language', 'maritalstatus']  \n",
    "\n",
    "all_metrics = []\n",
    "\n",
    "for i, attr_name in enumerate(attribute_names):\n",
    "    prot_attr = protected_attributes[:, i]\n",
    "    priv_group = priviliged_groups[i]\n",
    "    \n",
    "    try:\n",
    "        metrics = get_group_metrics(\n",
    "            y_true=y_true,\n",
    "            y_pred=None,  \n",
    "            prot_attr=prot_attr,\n",
    "            priv_group=priv_group,\n",
    "            pos_label=1\n",
    "        )\n",
    "        metrics['attribute'] = attr_name\n",
    "        all_metrics.append(metrics)\n",
    "    except Exception as e:\n",
    "        print(f\"[{attr_name}] Error computing metrics: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03a16b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics = pd.DataFrame(all_metrics)\n",
    "df_melted = df_metrics.melt(id_vars='attribute', var_name='metric', value_name='value')\n",
    "df_melted = df_melted.dropna(subset=['value'])\n",
    "fig = go.Figure()\n",
    "\n",
    "for metric_name in df_melted['metric'].unique():\n",
    "    if metric_name == 'attribute':\n",
    "        continue\n",
    "    df_metric = df_melted[df_melted['metric'] == metric_name]\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=df_metric['attribute'],\n",
    "        y=df_metric['value'],\n",
    "        name=metric_name\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    barmode='group',\n",
    "    title='Fairness Metrics by Protected Attribute',\n",
    "    xaxis_title='Protected Attribute',\n",
    "    yaxis_title='Metric Value',\n",
    "    legend_title='Metric'\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c6599a",
   "metadata": {},
   "source": [
    "Loaded test set with predictions and prepared for fairness analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50aa0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "protected_attributes =  ['gender', 'race', 'ethnicity', 'language', 'maritalstatus']  \n",
    "\n",
    "def compute_fairness_metrics_from_csv(\n",
    "    csv_path,\n",
    "    label_column,\n",
    "    protected_attributes,\n",
    "    attribute_names,\n",
    "    priviliged_groups\n",
    "):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df['filename'] = df['filename'].apply(lambda x: x.split(\".\")[0])\n",
    "    # Drop columns if they exist\n",
    "    drop_cols = [col for col in ['slo_fundus', 'age', 'note', 'gpt4_summary'] if col in df.columns]\n",
    "    df = df.drop(columns=drop_cols)\n",
    "    df_numeric, _ = convert_to_numerical_with_encodings(df)\n",
    "    dataset = create_binary_label_dataset(df_numeric, label_column, protected_attributes)\n",
    "    protected_attrs = dataset.protected_attributes\n",
    "    y_true = dataset.labels\n",
    "    y_pred = df_numeric['pred'].values\n",
    "\n",
    "    all_metrics = []\n",
    "    for i, attr_name in enumerate(protected_attributes):\n",
    "        prot_attr = protected_attrs[:, i]\n",
    "        priv_group = priviliged_groups[i]\n",
    "        try:\n",
    "            metrics = get_group_metrics(\n",
    "                y_true=y_true,\n",
    "                y_pred=y_pred,\n",
    "                prot_attr=prot_attr,\n",
    "                priv_group=priv_group,\n",
    "                pos_label=1\n",
    "            )\n",
    "            metrics['attribute'] = attr_name\n",
    "            all_metrics.append(metrics)\n",
    "        except Exception as e:\n",
    "            print(f\"[{attr_name}] Error computing metrics: {e}\")\n",
    "    return pd.DataFrame(all_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e55ce19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics_preds = compute_fairness_metrics_from_csv(\n",
    "    \"./medical/df_test_with_preds.csv\",\n",
    "    label_column,\n",
    "    protected_attributes,\n",
    "    attribute_names,\n",
    "    priviliged_groups\n",
    ")\n",
    "df_melted_preds = df_metrics_preds.melt(id_vars='attribute', var_name='metric', value_name='value')\n",
    "df_melted_preds = df_melted_preds.dropna(subset=['value'])\n",
    "fig_preds = go.Figure()\n",
    "for metric_name in df_melted_preds['metric'].unique():\n",
    "    if metric_name == 'attribute':\n",
    "        continue\n",
    "    df_metric = df_melted_preds[df_melted_preds['metric'] == metric_name]\n",
    "    fig_preds.add_trace(go.Bar(\n",
    "        x=df_metric['attribute'],\n",
    "        y=df_metric['value'],\n",
    "        name=metric_name\n",
    "    ))\n",
    "fig_preds.update_layout(\n",
    "    barmode='group',\n",
    "    title='Fairness Metrics by Protected Attribute (with Predictions)',\n",
    "    xaxis_title='Protected Attribute',\n",
    "    yaxis_title='Metric Value',\n",
    "    legend_title='Metric',\n",
    "    width=1200,\n",
    "    height=700\n",
    ")\n",
    "fig_preds.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de5b403",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.colors\n",
    "\n",
    "common_metrics = set(df_melted['metric']).intersection(set(df_melted_preds['metric']))\n",
    "df_before = df_melted[df_melted['metric'].isin(common_metrics)]\n",
    "df_after = df_melted_preds[df_melted_preds['metric'].isin(common_metrics)]\n",
    "\n",
    "metric_names = list(common_metrics)\n",
    "palette = plotly.colors.qualitative.Plotly  # or use another palette if you prefer\n",
    "color_map = {metric: palette[i % len(palette)] for i, metric in enumerate(metric_names)}\n",
    "\n",
    "fig_compare = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=[\"Fairness Metrics Before Predictions\", \"Fairness Metrics After Predictions\"],\n",
    "    shared_yaxes=True\n",
    ")\n",
    "\n",
    "for metric in metric_names:\n",
    "    df_b = df_before[df_before['metric'] == metric]\n",
    "    df_a = df_after[df_after['metric'] == metric]\n",
    "    color = color_map[metric]\n",
    "    fig_compare.add_trace(\n",
    "        go.Bar(\n",
    "            x=df_b['attribute'],\n",
    "            y=df_b['value'],\n",
    "            name=metric,\n",
    "            legendgroup=metric,\n",
    "            showlegend=True,\n",
    "            marker_color=color\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    fig_compare.add_trace(\n",
    "        go.Bar(\n",
    "            x=df_a['attribute'],\n",
    "            y=df_a['value'],\n",
    "            name=metric,\n",
    "            legendgroup=metric,\n",
    "            showlegend=False,\n",
    "            marker_color=color\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "\n",
    "fig_compare.update_layout(\n",
    "    height=500,\n",
    "    width=1100,\n",
    "    barmode='group',\n",
    "    title_text=\"Comparison of Fairness Metrics Before and After Predictions\",\n",
    "    legend_title=\"Metric\"\n",
    ")\n",
    "fig_compare.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7d55d4",
   "metadata": {},
   "source": [
    "loading the fine tunned version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5238c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_after = df_metrics_preds.melt(id_vars='attribute', var_name='metric', value_name='value')\n",
    "df_after = df_after.dropna(subset=['value'])\n",
    "\n",
    "df_metrics_fine_tuned = compute_fairness_metrics_from_csv(\n",
    "    \"./medical/df_fine_tuned_preds.csv\",\n",
    "    label_column,\n",
    "    protected_attributes,\n",
    "    attribute_names,\n",
    "    priviliged_groups\n",
    ")\n",
    "df_fine_tuned = df_metrics_fine_tuned.melt(id_vars='attribute', var_name='metric', value_name='value')\n",
    "df_fine_tuned = df_fine_tuned.dropna(subset=['value'])\n",
    "\n",
    "common_metrics = sorted(list(set(df_after['metric']).intersection(df_fine_tuned['metric'])))\n",
    "metric_names = list(common_metrics)\n",
    "palette = plotly.colors.qualitative.Plotly\n",
    "color_map = {metric: palette[i % len(palette)] for i, metric in enumerate(metric_names)}\n",
    "\n",
    "\n",
    "fig_compare = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=[\n",
    "        \"Fairness Metrics After Predictions\",\n",
    "        \"Fairness Metrics After Fine-Tuning\"\n",
    "    ],\n",
    "    shared_yaxes=True\n",
    ")\n",
    "\n",
    "for metric in metric_names:\n",
    "    df_a = df_after[df_after['metric'] == metric]\n",
    "    df_f = df_fine_tuned[df_fine_tuned['metric'] == metric]\n",
    "    color = color_map[metric]\n",
    "    fig_compare.add_trace(\n",
    "        go.Bar(\n",
    "            x=df_a['attribute'],\n",
    "            y=df_a['value'],\n",
    "            name=metric,\n",
    "            legendgroup=metric,\n",
    "            showlegend=True,\n",
    "            marker_color=color\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    fig_compare.add_trace(\n",
    "        go.Bar(\n",
    "            x=df_f['attribute'],\n",
    "            y=df_f['value'],\n",
    "            name=metric,\n",
    "            legendgroup=metric,\n",
    "            showlegend=False,\n",
    "            marker_color=color\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "\n",
    "fig_compare.update_layout(\n",
    "    height=500,\n",
    "    width=1200,\n",
    "    barmode='group',\n",
    "    title_text=\"Comparison of Fairness Metrics: After Predictions vs. After Fine-Tuning\",\n",
    "    legend_title=\"Metric\"\n",
    ")\n",
    "fig_compare.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ef92bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute fairness metrics for the fair predictions\n",
    "df_metrics_fair = compute_fairness_metrics_from_csv(\n",
    "    \"./medical/df_fair_preds_for_race.csv\",\n",
    "    label_column,\n",
    "    protected_attributes,\n",
    "    attribute_names,\n",
    "    priviliged_groups\n",
    ")\n",
    "df_fair = df_metrics_fair.melt(id_vars='attribute', var_name='metric', value_name='value')\n",
    "df_fair = df_fair.dropna(subset=['value'])\n",
    "\n",
    "# Use the already computed fine-tuned metrics: df_metrics_fine_tuned\n",
    "df_fine_tuned = df_metrics_fine_tuned.melt(id_vars='attribute', var_name='metric', value_name='value')\n",
    "df_fine_tuned = df_fine_tuned.dropna(subset=['value'])\n",
    "\n",
    "# Find common metrics\n",
    "common_metrics = sorted(list(set(df_fine_tuned['metric']).intersection(df_fair['metric'])))\n",
    "metric_names = list(common_metrics)\n",
    "import plotly.colors\n",
    "palette = plotly.colors.qualitative.Plotly\n",
    "color_map = {metric: palette[i % len(palette)] for i, metric in enumerate(metric_names)}\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "fig_compare = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=[\n",
    "        \"Fairness Metrics After Fine-Tuning\",\n",
    "        \"Fairness Metrics After Fair Model\"\n",
    "    ],\n",
    "    shared_yaxes=True\n",
    ")\n",
    "\n",
    "for metric in metric_names:\n",
    "    df_f = df_fine_tuned[df_fine_tuned['metric'] == metric]\n",
    "    df_fair_m = df_fair[df_fair['metric'] == metric]\n",
    "    color = color_map[metric]\n",
    "    fig_compare.add_trace(\n",
    "        go.Bar(\n",
    "            x=df_f['attribute'],\n",
    "            y=df_f['value'],\n",
    "            name=metric,\n",
    "            legendgroup=metric,\n",
    "            showlegend=True,\n",
    "            marker_color=color\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    fig_compare.add_trace(\n",
    "        go.Bar(\n",
    "            x=df_fair_m['attribute'],\n",
    "            y=df_fair_m['value'],\n",
    "            name=metric,\n",
    "            legendgroup=metric,\n",
    "            showlegend=False,\n",
    "            marker_color=color\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "\n",
    "fig_compare.update_layout(\n",
    "    height=500,\n",
    "    width=1200,\n",
    "    barmode='group',\n",
    "    title_text=\"Comparison of Fairness Metrics: Fine-Tuned vs. Fair Model\",\n",
    "    legend_title=\"Metric\"\n",
    ")\n",
    "fig_compare.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f758bfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ideal_fairness_metrics = {\n",
    "    \"statistical_parity_difference\": 0.0,\n",
    "    \"disparate_impact_ratio\": 1.0,\n",
    "    \"equal_opportunity_difference\": 0.0,\n",
    "    \"average_odds_difference\": 0.0,\n",
    "    \"conditional_demographic_disparity\": 0.0,\n",
    "    \"smoothed_edf\": 1.0,\n",
    "    \"df_bias_amplification\": 0.0\n",
    "}\n",
    "\n",
    "def compute_distance_to_ideal(df_metrics):\n",
    "    # Prepare a list to collect results\n",
    "    results = []\n",
    "    for _, row in df_metrics.iterrows():\n",
    "        row_result = {'attribute': row['attribute']}\n",
    "        total_dist = 0.0\n",
    "        for metric, ideal_value in ideal_fairness_metrics.items():\n",
    "            if metric in row:\n",
    "                sub_dist = (ideal_value - row[metric]) ** 2\n",
    "                row_result[f'dist_{metric}'] = np.sqrt(sub_dist)\n",
    "                total_dist += sub_dist\n",
    "            else:\n",
    "                row_result[f'dist_{metric}'] = np.nan\n",
    "        row_result['total_distance'] = np.sqrt(total_dist)\n",
    "        results.append(row_result)\n",
    "    return pd.DataFrame(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff5db82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute distances to ideal for both models\n",
    "df_distance_fine_tuned = compute_distance_to_ideal(df_metrics_fine_tuned)\n",
    "df_distance_fair = compute_distance_to_ideal(df_metrics_fair)\n",
    "\n",
    "# List of metrics to plot (excluding 'attribute' and 'total_distance')\n",
    "metrics = [col for col in df_distance_fine_tuned.columns if col.startswith('dist_') and col != 'total_distance']\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Plot sub-distances for each metric\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=len(metrics),\n",
    "    subplot_titles=[m.replace('dist_', '') for m in metrics],\n",
    "    shared_yaxes=False\n",
    ")\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=df_distance_fine_tuned['attribute'],\n",
    "            y=df_distance_fine_tuned[metric],\n",
    "            name='Fine-Tuned',\n",
    "            marker_color='royalblue',\n",
    "            showlegend=(i==0)\n",
    "        ),\n",
    "        row=1, col=i+1\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=df_distance_fair['attribute'],\n",
    "            y=df_distance_fair[metric],\n",
    "            name='Fair Model',\n",
    "            marker_color='orange',\n",
    "            showlegend=(i==0)\n",
    "        ),\n",
    "        row=1, col=i+1\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    height=400,\n",
    "    width=300*len(metrics),\n",
    "    title_text=\"Distance to Ideal for Each Fairness Metric and Attribute\",\n",
    "    barmode='group'\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# Plot total distance for each attribute\n",
    "fig_total = go.Figure()\n",
    "fig_total.add_trace(go.Bar(\n",
    "    x=df_distance_fine_tuned['attribute'],\n",
    "    y=df_distance_fine_tuned['total_distance'],\n",
    "    name='Fine-Tuned',\n",
    "    marker_color='royalblue'\n",
    "))\n",
    "fig_total.add_trace(go.Bar(\n",
    "    x=df_distance_fair['attribute'],\n",
    "    y=df_distance_fair['total_distance'],\n",
    "    name='Fair Model',\n",
    "    marker_color='orange'\n",
    "))\n",
    "fig_total.update_layout(\n",
    "    title=\"Total Distance to Ideal for Each Attribute\",\n",
    "    xaxis_title=\"Protected Attribute\",\n",
    "    yaxis_title=\"Total Distance\",\n",
    "    barmode='group',\n",
    "    width=700,\n",
    "    height=400\n",
    ")\n",
    "fig_total.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0318558a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed2dae5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
