{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "psFkRRw6CHO1",
        "outputId": "9d42d394-5bbf-459c-b977-5eefde0a35c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.11/dist-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown) (4.13.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown) (3.18.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.11/dist-packages (from gdown) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gdown) (4.67.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2025.4.26)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=12oJbYhO8losIkZirQUqkbJGtl7WvstH-\n",
            "From (redirected): https://drive.google.com/uc?id=12oJbYhO8losIkZirQUqkbJGtl7WvstH-&confirm=t&uuid=28b935b4-2ab6-4c64-a527-152540839e15\n",
            "To: /content/dataset.zip\n",
            "100%|██████████| 2.68G/2.68G [00:25<00:00, 106MB/s] \n"
          ]
        }
      ],
      "source": [
        "!pip install -U gdown\n",
        "import gdown\n",
        "file_id = \"12oJbYhO8losIkZirQUqkbJGtl7WvstH-\"\n",
        "output = \"dataset.zip\"\n",
        "gdown.download(f\"https://drive.google.com/uc?id={file_id}\", output, quiet=False)\n",
        "\n",
        "import zipfile\n",
        "with zipfile.ZipFile(\"dataset.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"dataset\")\n",
        "\n",
        "file_id = \"1ey9CpldpHXRWH3jMzveW9K6tYyfCpNuy\"\n",
        "output = \"metadata_summary.csv\"\n",
        "gdown.download(f\"https://drive.google.com/uc?id={file_id}\", output, quiet=False)\n",
        "\n",
        "!ls -l\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZvlwjyVCu9z"
      },
      "outputs": [],
      "source": [
        "! pip install ftfy regex tqdm\n",
        "! pip install git+https://github.com/openai/CLIP.git\n",
        "! pip install aif360 fairlearn\n",
        "! pip install umap-learn\n",
        "! pip install umap-learn[plot]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yVa4KQgfDzX4"
      },
      "outputs": [],
      "source": [
        "#importation des métas-donénes + labels\n",
        "import pandas as pd\n",
        "\n",
        "df_summary = pd.read_csv(\"metadata_summary.csv\")\n",
        "df_summary[\"filename\"] = df_summary[\"filename\"].apply(lambda x: x.split(\".\")[0])\n",
        "df_summary.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "outy3EHlRdYQ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "FOLDER_PATH = './dataset/Test'\n",
        "\n",
        "\n",
        "IMAGE_EXTENSIONS = ['.png', '.jpg', '.jpeg']\n",
        "data_list = []\n",
        "\n",
        "for filename in os.listdir(FOLDER_PATH):\n",
        "    if filename.endswith('.npz'):\n",
        "        file_stem = os.path.splitext(filename)[0]\n",
        "        npz_path = os.path.join(FOLDER_PATH, filename)\n",
        "        npz_data = np.load(npz_path, allow_pickle=True)\n",
        "        id = file_stem.split(\"_\")[-1]\n",
        "\n",
        "\n",
        "        data_entry = {\n",
        "            \"filename\": file_stem,\n",
        "            \"slo_fundus\": npz_data[\"slo_fundus\"],\n",
        "        }\n",
        "\n",
        "        data_list.append(data_entry)\n",
        "\n",
        "print(f\"Loaded {len(data_list)} entries successfully.\")\n",
        "df = pd.DataFrame(data_list)\n",
        "df = pd.merge(df, df_summary, on='filename', how='inner').drop(columns = [\"use\"])\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import clip\n",
        "import torch\n",
        "from PIL import Image, ImageEnhance, ImageFilter\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "from umap import UMAP\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "from sklearn.metrics import roc_auc_score # Import roc_auc_score here\n"
      ],
      "metadata": {
        "id": "uzLH7UECdpPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "fbZFJG1SqdKf"
      },
      "outputs": [],
      "source": [
        "class RunCLIP:\n",
        "  def __init__(self, images, model_name=\"ViT-B/32\"):\n",
        "    self.images = images\n",
        "    self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    self.model, self.preprocess = clip.load(model_name, self.device)\n",
        "    self.model_name = model_name\n",
        "    self.run_result = None\n",
        "    self.features = None\n",
        "\n",
        "  def setClasses(self, classes_dict):\n",
        "    self.classes_dict = classes_dict\n",
        "    self.classes = []\n",
        "    self.yes_class_index = -1\n",
        "    for cat in classes_dict.values():\n",
        "      for idx, c in enumerate(cat):\n",
        "        self.classes.append(c[1])\n",
        "        if c[1] == \"yes\":\n",
        "            self.yes_class_index = len(self.classes) - 1\n",
        "\n",
        "  def setModel(self, model):\n",
        "    self.model = model\n",
        "    self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    self.model.to(self.device)\n",
        "\n",
        "\n",
        "  def _encode_image(self, image):\n",
        "    image_input = self.preprocess(Image.fromarray(image.astype(np.uint8))).unsqueeze(0).to(self.device)\n",
        "    with torch.no_grad():\n",
        "        image_features = self.model.encode_image(image_input)\n",
        "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "    return image_features\n",
        "\n",
        "  def _encode_text(self, text_list):\n",
        "      text_inputs = torch.cat([clip.tokenize(text, truncate = True) for text in text_list]).to(self.device)\n",
        "      with torch.no_grad():\n",
        "          text_features = self.model.encode_text(text_inputs)\n",
        "          text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "      return text_features\n",
        "\n",
        "  def _get_text_classes(self):\n",
        "      return [f\"{c[0]}\" for category_list in self.classes_dict.values() for c in category_list]\n",
        "\n",
        "  def _compute_similarity_with_metadata(self, image_features, metadata_text):\n",
        "    text_meta_input = clip.tokenize([metadata_text]).to(self.device)\n",
        "    with torch.no_grad():\n",
        "      text_meta_features = self.model.encode_text(text_meta_input)\n",
        "      text_meta_features /= text_meta_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "    text_classes = self._get_text_classes()\n",
        "    text_features = self._encode_text(text_classes)\n",
        "\n",
        "    img_similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
        "    meta_similarity = (100.0 * text_meta_features @ text_features.T).softmax(dim=-1)\n",
        "\n",
        "    return torch.max(img_similarity,meta_similarity)\n",
        "\n",
        "  def _compute_similarity_standard(self, image_features):\n",
        "      text_classes = self._get_text_classes()\n",
        "      text_features = self._encode_text(text_classes)\n",
        "      return (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
        "\n",
        "  def _get_top_predictions(self, similarity, topk):\n",
        "      values, indices = similarity[0].topk(topk)\n",
        "      return values.cpu().detach().numpy(), indices.cpu().detach().numpy()\n",
        "\n",
        "  def _create_description(self, indices, similarity, topk):\n",
        "      desc = {k: None for k in self.classes_dict.keys()}\n",
        "\n",
        "      for j in range(topk):\n",
        "          predicted_class = self.classes[indices[j].item()]\n",
        "          for k in desc.keys():\n",
        "              if predicted_class in [c[1] for c in self.classes_dict[k]] and desc[k] is None:\n",
        "                  desc[k] = predicted_class\n",
        "\n",
        "      desc[\"similarity_score\"] = similarity[0][self.yes_class_index].cpu().item()\n",
        "      return desc\n",
        "\n",
        "  def _plot_single_image_results(self, image, top_probs, top_classes, image_idx, n_rows, n_cols):\n",
        "      plt.subplot(n_rows, n_cols, 2 * image_idx + 1)\n",
        "      plt.imshow(image)\n",
        "      plt.axis(\"off\")\n",
        "\n",
        "      plt.subplot(n_rows, n_cols, 2 * image_idx + 2)\n",
        "      y = np.arange(top_probs.shape[-1])\n",
        "      plt.grid()\n",
        "      plt.barh(y, top_probs)\n",
        "      plt.gca().invert_yaxis()\n",
        "      plt.gca().set_axisbelow(True)\n",
        "      plt.yticks(y, [self.classes[index] for index in top_classes])\n",
        "      plt.xlabel(\"probability\")\n",
        "\n",
        "  def runPreds(self, topk=8, plot_preview=True, metadata_text=None):\n",
        "      if metadata_text is not None:\n",
        "          assert len(metadata_text) == len(self.images), \\\n",
        "          \"Metadata text must have the same length as images.\"\n",
        "      else:\n",
        "          if not hasattr(self, 'classes'):\n",
        "              raise ValueError(\"Classes not set. Call setClasses() first.\")\n",
        "          if self.yes_class_index == -1:\n",
        "              raise ValueError(\"Could not find the 'yes' class index. Ensure 'yes' is in your classes_dict values.\")\n",
        "\n",
        "      topk = min(topk, len(self.classes))\n",
        "\n",
        "      all_image_features = []\n",
        "      images_desc = []\n",
        "      all_top_probs = []\n",
        "      all_top_classes = []\n",
        "\n",
        "      for i, image in enumerate(self.images):\n",
        "          image_features = self._encode_image(image)\n",
        "          all_image_features.append(image_features.cpu().numpy().squeeze())\n",
        "\n",
        "          if metadata_text is not None:\n",
        "              similarity = self._compute_similarity_with_metadata(image_features, metadata_text[i])\n",
        "          else:\n",
        "              similarity = self._compute_similarity_standard(image_features)\n",
        "\n",
        "          top_probs, top_classes = self._get_top_predictions(similarity, topk)\n",
        "          all_top_probs.append(top_probs)\n",
        "          all_top_classes.append(top_classes)\n",
        "\n",
        "          desc = self._create_description(top_classes, similarity, topk)\n",
        "          images_desc.append(desc)\n",
        "\n",
        "      if plot_preview:\n",
        "          num_images = min(10, len(self.images))\n",
        "\n",
        "          plt.figure(figsize=(16, 16))\n",
        "          n_cols = 4\n",
        "          n_rows = (num_images + 1) // 2\n",
        "\n",
        "          for i in range(num_images):\n",
        "              self._plot_single_image_results(\n",
        "                  self.images[i], all_top_probs[i], all_top_classes[i],\n",
        "                  i, n_rows, n_cols\n",
        "              )\n",
        "\n",
        "          plt.subplots_adjust(wspace=0.5, hspace=0.5)\n",
        "          plt.show()\n",
        "\n",
        "      self.features = np.vstack(all_image_features)\n",
        "      self.run_result = pd.DataFrame(images_desc)\n",
        "\n",
        "  def export_to_csv(self, path):\n",
        "      if self.run_result is None:\n",
        "          raise ValueError(\"runPreds() has not been called yet.\")\n",
        "      self.run_result.to_csv(path, index=False)\n",
        "\n",
        "  def plot_latent_space(self, label_col: str, n_neighbors: int = 3, min_dist: float = 0.1, random_state: int = 42):\n",
        "      if self.run_result is None or self.features is None:\n",
        "          raise ValueError(\"You must call runPreds() first.\")\n",
        "\n",
        "      if label_col not in self.run_result.columns:\n",
        "          raise ValueError(f\"Column '{label_col}' not found in run_result DataFrame.\")\n",
        "\n",
        "      X = self.features\n",
        "      y = self.run_result[label_col].values\n",
        "\n",
        "      valid_mask = pd.notnull(y)\n",
        "      X = X[valid_mask]\n",
        "      y = y[valid_mask]\n",
        "\n",
        "      if len(X) == 0:\n",
        "          print(\"No valid data points found for plotting latent space.\")\n",
        "          return\n",
        "\n",
        "      X_scaled = StandardScaler().fit_transform(X)\n",
        "      reducer = UMAP(n_neighbors=n_neighbors, min_dist=min_dist, random_state=random_state)\n",
        "      embedding = reducer.fit_transform(X_scaled)\n",
        "      unique_labels = np.unique(y)\n",
        "\n",
        "      if len(unique_labels) > 20:\n",
        "          print(f\"Warning: Too many unique labels ({len(unique_labels)}). Plotting may not be clear.\")\n",
        "\n",
        "      cmap = plt.cm.get_cmap('tab20', len(unique_labels))\n",
        "      color_map = {lbl: cmap(i) for i, lbl in enumerate(unique_labels)}\n",
        "\n",
        "      plt.figure(figsize=(10, 8))\n",
        "      for lbl in unique_labels:\n",
        "          mask_lbl = (y == lbl)\n",
        "          plt.scatter(embedding[mask_lbl, 0], embedding[mask_lbl, 1],\n",
        "                    s=15, color=color_map[lbl], label=lbl, alpha=0.8)\n",
        "\n",
        "      plt.gca().set_aspect('equal', 'datalim')\n",
        "      plt.legend(title=label_col, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "      plt.title(f'UMAP of CLIP features colored by {label_col}', fontsize=14)\n",
        "      plt.xlabel('UMAP 1', fontsize=12)\n",
        "      plt.ylabel('UMAP 2', fontsize=12)\n",
        "      plt.tight_layout()\n",
        "      plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o6R6lBDO5o93"
      },
      "outputs": [],
      "source": [
        "# test = RunCLIP(list(df[\"slo_fundus\"]))\n",
        "# test.setClasses({\n",
        "#    \"pred\": [(\"a medical picture of a person with glaucoma\", \"yes\") , (\"a medical picture of a person without glaucoma\", \"no\")]\n",
        "# })\n",
        "# test.runPreds()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_8THWwz7vZh"
      },
      "outputs": [],
      "source": [
        "# test.plot_latent_space(\"pred\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zb1vc1PWD9De"
      },
      "outputs": [],
      "source": [
        "# merge = df.copy().drop(columns = [c for c in df.columns if c != \"glaucoma\"])\n",
        "# merge[\"pred\"] = test.run_result[\"pred\"]\n",
        "# print(f\"acc : { (merge['glaucoma'] == merge['pred']).sum() / len(merge)*100 : .2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NuX98L5bdJWk"
      },
      "source": [
        "### Avec meta-donneés"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lN3xy9z4keW5",
        "outputId": "aacc1bc5-6796-4124-cec3-43c507bd31d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.2)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.52.4-py3-none-any.whl.metadata (38 kB)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
            "Downloading transformers-4.52.4-py3-none-any.whl (10.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m67.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.52.2\n",
            "    Uninstalling transformers-4.52.2:\n",
            "      Successfully uninstalled transformers-4.52.2\n",
            "Successfully installed transformers-4.52.4\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.46.0-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: torch<3,>=2.2 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.2->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\n",
            "Downloading bitsandbytes-0.46.0-py3-none-manylinux_2_24_x86_64.whl (67.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.46.0\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade transformers accelerate\n",
        "!pip install bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import CLIPTokenizer, AutoTokenizer, AutoModelForSeq2SeqLM, pipeline"
      ],
      "metadata": {
        "id": "A-qNtRiAdxbq"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "4bmX9d-kdI9c"
      },
      "outputs": [],
      "source": [
        "if False: #if you wanna regenerate\n",
        "  clip_tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "  model_id = \"Falconsai/text_summarization\"\n",
        "  summarizer_tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "  model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
        "  pipe = pipeline(\"summarization\", model=model, tokenizer=summarizer_tokenizer, device=-1)\n",
        "\n",
        "  def needs_summarization(text, limit=72):\n",
        "      clip_tokens = clip_tokenizer(text, return_tensors=\"pt\", truncation=False, padding=False)\n",
        "      return clip_tokens[\"input_ids\"].shape[1] > limit\n",
        "\n",
        "  def summarize(note):\n",
        "      # if not needs_summarization(note):\n",
        "      #     return note\n",
        "      prompt = f\"summarize and remove all mention of glaucoma: {note}\"\n",
        "      try:\n",
        "          result = pipe(prompt, max_new_tokens=45, min_length=20, do_sample=False)[0]\n",
        "          return result[\"summary_text\"].strip()\n",
        "      except Exception as e:\n",
        "          print(f\"Error summarizing note: {e}\")\n",
        "          return \"SUMMARY_FAILED\"\n",
        "\n",
        "  summaries = [summarize(note) for note in df[\"gpt4_summary\"]]\n",
        "else:\n",
        "  with open(\"medical_notes_summaries_v1.txt\", \"r\") as f:\n",
        "    summaries = f.read().strip().split(\"\\\\\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "mlXAOGzRzfNQ"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import re\n",
        "\n",
        "with open(\"medical_notes_summaries_v1.txt\", \"w\") as f:\n",
        "    f.write(\"\\\\\".join(summaries))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# files.download(\"medical_notes_summaries_v1.txt\")"
      ],
      "metadata": {
        "id": "sdC0vuKavkDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_spaces_near_punctuation(text):\n",
        "    text = re.sub(r'\\s+([.,;:!?])', r'\\1', text)\n",
        "    text = re.sub(r'([.,;:!?])\\s+', r'\\1', text)\n",
        "    return text\n",
        "\n",
        "\n",
        "summaries = []\n",
        "try:\n",
        "    with open(\"medical_notes_summaries_v1.txt\", \"r\") as f:\n",
        "        content = f.read().strip()\n",
        "        summaries = content.split(\"\\\\\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: medical_notes_summaries_v1.text not found. Please generate it first.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "\n",
        "for i in range(len(summaries)):\n",
        "  summaries[i] = remove_spaces_near_punctuation(summaries[i])\n",
        "\n",
        "print(f\"Loaded {len(summaries)} summaries.\")"
      ],
      "metadata": {
        "id": "k-UosvpevmVF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87743aaf-20c2-44b3-ff0a-1599639b9fac"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 1 summaries.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TfN3D_vrQ_CA"
      },
      "outputs": [],
      "source": [
        "clip_runner = RunCLIP(list(df[\"slo_fundus\"]))\n",
        "clip_runner.setClasses({\n",
        "   \"pred\": [(\"A person diagnosed with glaucoma based on clinical findings.\", \"yes\") ,\n",
        "    (\"A person with no signs of glaucoma; vision appears normal.\", \"no\")]\n",
        "})\n",
        "clip_runner.runPreds(metadata_text=summaries)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6SfTnlJ32Pr"
      },
      "outputs": [],
      "source": [
        "clip_runner.plot_latent_space(\"pred\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77YnSjUH4HFB"
      },
      "outputs": [],
      "source": [
        "merge = df.copy().drop(columns = [c for c in df.columns if c != \"glaucoma\"])\n",
        "merge[\"pred\"] = clip_runner.run_result[\"pred\"]\n",
        "print(f\"acc : { (merge['glaucoma'] == merge['pred']).sum() / len(merge)*100 : .2f}\")\n",
        "print(\"Number of yes predictions :\" ,(clip_runner.run_result[\"pred\"]==\"yes\").sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hSNAcS7VSPSz"
      },
      "outputs": [],
      "source": [
        "resultat = clip_runner.run_result.copy()\n",
        "resultat[\"label\"] = df[\"glaucoma\"]\n",
        "resultat[\"race\"] = df[\"race\"]\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "for race, group_df in resultat.groupby(\"race\"):\n",
        "    auc = roc_auc_score(group_df[\"label\"], group_df[\"similarity_score\"])\n",
        "    print(f\"[{race}] AUC: {auc:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"pred\"] = clip_runner.run_result[\"pred\"]\n",
        "df.to_csv(\"df_test_with_preds.csv\", index=False)"
      ],
      "metadata": {
        "id": "HBBMsGhqq9eW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine Tunning (ça arrive fort)"
      ],
      "metadata": {
        "id": "PCVuUof2oa2g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ftfy regex tqdm\n",
        "!pip install torchvision"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtTjX8qDa-0D",
        "outputId": "1b475c6d-b99e-468e-c72f-e5665bb9e747"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.11/dist-packages (6.3.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy) (0.2.13)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: torch==2.6.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.6.0+cu124)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0->torchvision) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0->torchvision) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "class GlaucomaDataset(Dataset):\n",
        "    def __init__(self, use_split, dataset_root, metadata_path, preprocess):\n",
        "        self.use_split = use_split.lower()\n",
        "        self.root_dir = os.path.join(dataset_root, use_split.capitalize())\n",
        "        self.annotations = pd.read_csv(metadata_path)\n",
        "        self.annotations[\"use\"] = self.annotations[\"use\"].str.strip().str.lower()\n",
        "        self.annotations = self.annotations[self.annotations[\"use\"] == self.use_split]\n",
        "        self.annotations = self.annotations[self.annotations[\"filename\"].str.lower().str.endswith('.npz')]\n",
        "        self.annotations = self.annotations.reset_index(drop=True)\n",
        "        self.preprocess = preprocess\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.annotations)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        npz_path = os.path.join(self.root_dir, self.annotations.iloc[idx][\"filename\"])\n",
        "        npz_data = np.load(npz_path, allow_pickle=True)\n",
        "        image_data = npz_data[\"slo_fundus\"]\n",
        "        image = self.preprocess(Image.fromarray(image_data.astype(np.uint8)).convert(\"RGB\"))\n",
        "        text = \"image of an eye with glaucoma\" if self.annotations.iloc[idx][\"glaucoma\"] == \"yes\"  else \"image of an eye without glaucoma\"     #text to associate to the image\n",
        "        return image, text\n",
        ""
      ],
      "metadata": {
        "id": "zOjfLkywbJvG"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "\n",
        "dataset_root = \"/content/dataset\"\n",
        "metadata_path = \"/content/metadata_summary.csv\"\n",
        "\n",
        "train_dataset = GlaucomaDataset(\"training\", dataset_root, metadata_path, preprocess)\n",
        "val_dataset   = GlaucomaDataset(\"validation\", dataset_root, metadata_path, preprocess)\n",
        "test_dataset  = GlaucomaDataset(\"test\", dataset_root, metadata_path, preprocess)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader   = DataLoader(val_dataset, batch_size=32)"
      ],
      "metadata": {
        "id": "fVYupa01xmt1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "451de09a-ce14-4656-d25d-f2b913b3ca5f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████| 338M/338M [00:08<00:00, 41.3MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "import clip\n",
        "\n",
        "# Fix temperature parameter BEFORE training\n",
        "if hasattr(model, 'logit_scale'):\n",
        "    print(f\"Original logit_scale: {model.logit_scale.exp().item():.4f}\")\n",
        "    # Reset to a much smaller, safer value\n",
        "    model.logit_scale.data.fill_(2.6592)  # ln(14.29) ≈ 2.6592, safer default\n",
        "    # Freeze the temperature parameter to prevent it from growing during training\n",
        "    model.logit_scale.requires_grad = False\n",
        "    print(f\"Fixed logit_scale: {model.logit_scale.exp().item():.4f}\")\n",
        "    print(\"Temperature parameter frozen during training\")\n",
        "\n",
        "# Ensure model is in training mode with gradients enabled\n",
        "model.train()\n",
        "\n",
        "# Freeze most of the model and only train the final layers\n",
        "for name, param in model.named_parameters():\n",
        "    param.requires_grad = False  # Freeze everything first\n",
        "\n",
        "# Only unfreeze the safest parameters (layer norms and final projections)\n",
        "trainable_params = []\n",
        "for name, param in model.named_parameters():\n",
        "    # Only train layer normalization parameters and avoid projection layers initially\n",
        "    if any(layer in name for layer in ['ln_final', 'ln_1', 'ln_2']) and 'weight' in name:\n",
        "        param.requires_grad = True\n",
        "        trainable_params.append(name)\n",
        "        print(f\"Training parameter: {name} - Shape: {param.shape}\")\n",
        "\n",
        "print(f\"Total trainable parameters: {len(trainable_params)}\")\n",
        "\n",
        "# If no trainable parameters found, add the safest ones\n",
        "if len(trainable_params) == 0:\n",
        "    print(\"No layer norm parameters found, using minimal set...\")\n",
        "    for name, param in model.named_parameters():\n",
        "        if 'ln_final' in name:\n",
        "            param.requires_grad = True\n",
        "            trainable_params.append(name)\n",
        "            print(f\"Training parameter: {name} - Shape: {param.shape}\")\n",
        "\n",
        "\n",
        "loss_img = nn.CrossEntropyLoss()\n",
        "loss_txt = nn.CrossEntropyLoss()\n",
        "\n",
        "trainable_params_list = [p for p in model.parameters() if p.requires_grad]\n",
        "if len(trainable_params_list) == 0:\n",
        "    print(\"WARNING: No trainable parameters found! Training will have no effect.\")\n",
        "    for name, param in model.named_parameters():\n",
        "        if 'ln_final' in name and 'weight' in name:\n",
        "            param.requires_grad = True\n",
        "            trainable_params_list.append(param)\n",
        "            break\n",
        "\n",
        "optimizer = optim.Adam(trainable_params_list, lr=1e-4, eps=1e-8)\n",
        "print(f\"Optimizer created with {len(trainable_params_list)} trainable parameters\")\n",
        "\n",
        "\n",
        "for epoch in range(5):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    num_batches = 0\n",
        "\n",
        "    for batch_idx, (images, texts) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        texts = clip.tokenize(texts).to(device)\n",
        "\n",
        "        logits_per_image, logits_per_text = model(images, texts)\n",
        "\n",
        "        logits_per_image = torch.clamp(logits_per_image, -20, 20)\n",
        "        logits_per_text = torch.clamp(logits_per_text, -20, 20)\n",
        "\n",
        "        if logits_per_image.abs().max() > 15 or logits_per_text.abs().max() > 15:\n",
        "            print(f\"Warning: Large logits detected - Image: {logits_per_image.abs().max():.2f}, Text: {logits_per_text.abs().max():.2f}\")\n",
        "\n",
        "\n",
        "        # Debug: Check logits range (remove after confirming fix works)\n",
        "        if batch_idx == 0:  # Only print for first batch to avoid spam\n",
        "            print(f\"Epoch {epoch+1}, Batch {batch_idx}: Logits range - Image: [{logits_per_image.min():.2f}, {logits_per_image.max():.2f}], Text: [{logits_per_text.min():.2f}, {logits_per_text.max():.2f}]\")\n",
        "\n",
        "        # Ground truth labels (diagonal matching)\n",
        "        ground_truth = torch.arange(len(images), device=device)\n",
        "\n",
        "        loss_img_val = loss_img(logits_per_image, ground_truth)\n",
        "        loss_txt_val = loss_txt(logits_per_text, ground_truth)\n",
        "        loss = (loss_img_val + loss_txt_val) / 2\n",
        "\n",
        "        if torch.isnan(logits_per_image).any() or torch.isnan(logits_per_text).any():\n",
        "            print(f\"NaN in logits detected at epoch {epoch+1}, batch {batch_idx}\")\n",
        "            break\n",
        "\n",
        "\n",
        "        if torch.isnan(loss):\n",
        "            print(f\"NaN loss detected at epoch {epoch+1}, batch {batch_idx}\")\n",
        "            print(f\"Loss img: {loss_img_val.item()}, Loss txt: {loss_txt_val.item()}\")\n",
        "            break\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # Check for NaN gradients before clipping (only check trainable params)\n",
        "        nan_grads = False\n",
        "        for name, param in model.named_parameters():\n",
        "            if param.requires_grad and param.grad is not None and torch.isnan(param.grad).any():\n",
        "                print(f\"NaN gradient detected in {name}\")\n",
        "                nan_grads = True\n",
        "                # Zero out the bad gradient instead of skipping\n",
        "                param.grad.zero_()\n",
        "                print(f\"Zeroed gradient for {name}\")\n",
        "\n",
        "        # Always continue with the update (even if we had to zero some gradients)\n",
        "\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)  # More aggressive clipping\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "        for name, param in model.named_parameters():\n",
        "            if param.requires_grad and torch.isnan(param).any():\n",
        "                print(f\"NaN parameter detected in {name} after update\")\n",
        "                # Reset this parameter to prevent cascade failure\n",
        "                nn.init.normal_(param.data, mean=0, std=0.01)\n",
        "                print(f\"Reset parameter {name} to prevent cascade failure\")\n",
        "                break\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        num_batches += 1\n",
        "\n",
        "\n",
        "    if num_batches > 0:\n",
        "        avg_loss = total_loss / num_batches\n",
        "        print(f\"Epoch {epoch + 1} - Loss: {avg_loss:.4f}\")\n",
        "    else:\n",
        "        print(f\"Epoch {epoch + 1} - Training stopped due to NaN\")\n",
        "        break"
      ],
      "metadata": {
        "collapsed": true,
        "id": "-IMlzuKvxpCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = RunCLIP(list(df[\"slo_fundus\"]))\n",
        "test.setModel(model)\n",
        "test.setClasses({\n",
        "   \"pred\": [(\"a medical picture of a person with glaucoma\", \"yes\") , (\"a medical picture of a person without glaucoma\", \"no\")]\n",
        "})\n",
        "test.runPreds()"
      ],
      "metadata": {
        "id": "MToEXyai1WQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test.plot_latent_space(\"pred\")"
      ],
      "metadata": {
        "id": "Y_2N4A1mHT5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merge = df.copy().drop(columns = [c for c in df.columns if c != \"glaucoma\"])\n",
        "merge[\"pred\"] = test.run_result[\"pred\"]\n",
        "print(f\"acc : { (merge['glaucoma'] == merge['pred']).sum() / len(merge)*100 : .2f}\")"
      ],
      "metadata": {
        "id": "Ct9U_fjFFVr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_fine_tuned_preds = df.copy()\n",
        "df_fine_tuned_preds[\"pred\"] = test.run_result[\"pred\"]\n",
        "df_fine_tuned_preds.to_csv(\"df_fine_tuned_preds.csv\", index=False)"
      ],
      "metadata": {
        "id": "acVcApOvHP2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Playing with the fine tunning process"
      ],
      "metadata": {
        "id": "ZZc_ZYiRyiMR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tqdm"
      ],
      "metadata": {
        "id": "mw-tXOf1Hm2r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "118eec8c-84d2-4a81-fa9c-6bd8f5784fba"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ici on va juste projeter les images dans l'espace latent puis train un reseau linéaire par dessus pour faire de la classification binaire"
      ],
      "metadata": {
        "id": "YEnpdh3L8Jbg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader\n",
        "import clip\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# classifier = nn.Sequential(\n",
        "#     nn.Linear(512, 512),\n",
        "#     nn.ReLU(),\n",
        "#     nn.Linear(512, 2)\n",
        "# ).float().to(device)\n",
        "\n",
        "# Pour que ca soit linéaire (à moins que je n'ai pas compris)\n",
        "classifier = nn.Linear(512, 2).float().to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(classifier.parameters(), lr=1e-4)\n",
        "\n",
        "def compute_accuracy(logits, labels):\n",
        "    preds = logits.argmax(dim=1)\n",
        "    return (preds == labels).sum().item(), labels.size(0)\n",
        "\n",
        "for epoch in range(30):\n",
        "    # Training loop\n",
        "    classifier.train()\n",
        "    train_loss = 0\n",
        "    train_correct = 0\n",
        "    train_total = 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images = images.to(device)\n",
        "        labels = torch.tensor([1 if label == 'image of an eye with glaucoma' else 0 for label in labels]).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            image_features = model.encode_image(images)\n",
        "        image_features = image_features.float()\n",
        "\n",
        "        logits = classifier(image_features)\n",
        "        loss = criterion(logits, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        correct, total = compute_accuracy(logits, labels)\n",
        "        train_correct += correct\n",
        "        train_total += total\n",
        "\n",
        "    train_acc = train_correct / train_total\n",
        "    avg_train_loss = train_loss / len(train_loader)\n",
        "\n",
        "    # Validation loop\n",
        "    classifier.eval()\n",
        "    val_loss = 0\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images = images.to(device)\n",
        "            labels = torch.tensor([1 if label == 'image of an eye with glaucoma' else 0 for label in labels]).to(device)\n",
        "\n",
        "            image_features = model.encode_image(images).float()\n",
        "            logits = classifier(image_features)\n",
        "            loss = criterion(logits, labels)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "            correct, total = compute_accuracy(logits, labels)\n",
        "            val_correct += correct\n",
        "            val_total += total\n",
        "\n",
        "    val_acc = val_correct / val_total\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "    print(f\"Epoch {epoch+1} | Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.4f} | Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "9cllqPBW6Dlo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Iz3au32hy4bH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}